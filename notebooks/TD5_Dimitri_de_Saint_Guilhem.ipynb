{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfJzTYrOG6A0",
        "outputId": "4d9addd3-1b1d-4b83-ecc7-2c27958024f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Apr 18 16:54:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   65C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hqrWZ8KLkZIJ"
      },
      "source": [
        "# TD 5: Transformers for computer vision\n",
        "By Nicolas Dufour, Vicky Kalogeiton and Pascal Vannier\n",
        "\n",
        "In this TD, we will implement the Transformers architecture. Transformers has been a key architecture in deep learning for the past 5 years.\n",
        "\n",
        "It has first began with NLP, then came audio and finally, since 2020, computer vision.\n",
        "We will implement every block that makes a transformer from scratch and we will try to create a deep understanding of what is happening.\n",
        "Here is a diagram for the transformer architecture:\n",
        "\n",
        "<img src=\"https://www.researchgate.net/profile/Miruna-Gheata/publication/355339249/figure/fig1/AS:1079476452622337@1634378650979/Encoder-decoder-architecture-of-the-Transformer-developed-by-Vaswani-et-al-28.ppm\" width=768>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF7b93xYkZIO"
      },
      "source": [
        "## Instructions\n",
        "As stated before, in pytorch you must achieve for loops at all cost. It's almost always possible to find a vectorized version of the operation you want to implement.\n",
        "In this TP, the only for-loop you can do is the training loop."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-Q4-KIMkZIQ",
        "outputId": "6f97d436-56e6-477e-81e0-30a29eb8cd27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting einops\n",
            "  Downloading einops-0.6.0-py3-none-any.whl (41 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.6/41.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting timm\n",
            "  Downloading timm-0.6.13-py3-none-any.whl (549 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m549.1/549.1 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.9/dist-packages (from timm) (6.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.9/dist-packages (from timm) (0.15.1+cu118)\n",
            "Collecting huggingface-hub\n",
            "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.1/200.1 kB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.7 in /usr/local/lib/python3.9/dist-packages (from timm) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (1.11.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from torch>=1.7->timm) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->torch>=1.7->timm) (16.0.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (23.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (4.65.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from huggingface-hub->timm) (2.27.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (8.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from torchvision->timm) (1.22.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch>=1.7->timm) (2.1.2)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (3.4)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->huggingface-hub->timm) (2022.12.7)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch>=1.7->timm) (1.3.0)\n",
            "Installing collected packages: huggingface-hub, timm\n",
            "Successfully installed huggingface-hub-0.13.4 timm-0.6.13\n"
          ]
        }
      ],
      "source": [
        "!pip install einops\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YrqLMlIzkZIS"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.optim import AdamW\n",
        "import math\n",
        "from torchvision.datasets import CIFAR10\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from einops import rearrange, repeat\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import requests\n",
        "from io import BytesIO\n",
        "from torchvision.models.feature_extraction import get_graph_node_names, create_feature_extractor\n",
        "import timm\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8nL9sVf3kZIU"
      },
      "source": [
        "## The Transformer model from the paper Attention is All You Need."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXmNHSpVkZIV"
      },
      "source": [
        "### Attention"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynYbNE2skZIW"
      },
      "source": [
        "The transformer architecture is built around one key block: The attention.\n",
        "The idea behind attention is the following. Imagine you want to retrieve information from a dictionary. The dictionnary is indexed by keys which maps to a particular value. Now, you have a query which will be matched against the keys of the dict and if you have a match, you will retrieve the associated value.\n",
        "Attention is very similar to this simple retrieval example. Now, with real data, we don't have this structure, we however are going to learn to create it.\n",
        "\n",
        "We have 2 sets of vectors (also named tokens). One is $X_{to}$ which is the destination set. We want to be able to map this set of tokens to queries. We achieve this by doing a linear projection of $X_{to}$. $Q = W_QX_{to}$\n",
        "\n",
        "The other set is $X_{from}$ the set from which we want to retrieve information. We will need to extract both keys and values from this set. We therefore do 2 linear projections of $X_{from}$. $K = W_KX_{from}$ and $V = W_VX_{from}$.\n",
        "\n",
        "Now, contrary to the dictionnary where queries and values are exact matchs, we don't have this here. Therefore, we will perform a softer match by computing the similarity matrix between $Q$ and $K$. Then for each $Q$, we want to output the values that have the higher similarity. We therefore output the weighted sum of the values, weighted by the softmax of the similarity (also called the attention matrix).\n",
        "\n",
        "Finally, the attention operation is given by the cross attention:\n",
        "\n",
        "$$\n",
        "A(Q,K,V) = SoftMax(\\frac{Q^TK}{\\sqrt{d_k}})V\n",
        "$$\n",
        "\n",
        "We divide the similarity by $\\sqrt{d_k}$ for stability reason to avoid the similarity to explode with big vectors which would lead to very sharp attention coeficients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-gDJuFhkZIY"
      },
      "source": [
        "##### Question 1:\n",
        "Implement the attention operation.\n",
        "\n",
        "Tip: Look into `torch.einsum` to easily compute the similarity matrix, an easy to understand explanation may be found [here](https://rockt.github.io/2018/04/30/einsum).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pSPzCFljkZIZ"
      },
      "outputs": [],
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim,):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        self.w_query = nn.Linear(x_to_dim, hidden_dim)\n",
        "        self.w_key = nn.Linear(x_from_dim,hidden_dim)\n",
        "        self.w_value = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.hidden_dim = hidden_dim\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        # To complete\n",
        "        query = self.w_query(x_to)\n",
        "        key = self.w_key(x_from)\n",
        "        value = self.w_value(x_from)\n",
        "\n",
        "        A = torch.einsum('bik,bjk->bij', query, key)\n",
        "        A = A/math.sqrt(self.hidden_dim)\n",
        "        A = nn.Softmax(dim = -1)(A)\n",
        "        A = torch.einsum('bik,bkj->bij', A, value)\n",
        "        return A"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p4ZKUNaH7h0F"
      },
      "outputs": [],
      "source": [
        "x = torch.Tensor([[[0,1,2,3],[1,2,3,4]]])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ke9L5VRQ7qLN"
      },
      "outputs": [],
      "source": [
        "attention = Attention(4,4,10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpljWEph7xGE",
        "outputId": "407c4b5b-5ea5-4a79-a688-e3416e556c4f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[-1.3018, -0.3023, -0.1118, -1.0333,  1.2913,  2.4931,  0.4207,\n",
              "           2.9757, -0.6286, -0.5660],\n",
              "         [-1.3028, -0.2606, -0.1687, -1.0282,  1.2700,  2.3313,  0.4122,\n",
              "           2.8274, -0.5908, -0.4989]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ],
      "source": [
        "attention(x,x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlTCLd-AkZIb"
      },
      "source": [
        "#### Multi-head attention\n",
        "\n",
        "We improve the above attention implementation by introducing mult-head attention. The idea here is that we compute the attention on subspaces of the $Q,K,V$ triplets.\n",
        "We split each vector in n subsets and compute the attention for each subset. At the end, we concatenate every attention output and project it with an output projection.\n",
        "\n",
        "##### Question 2\n",
        "Implement Multihead attention."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kNG9OtaLkZIc"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, x_to_dim, x_from_dim, hidden_dim, n_heads):\n",
        "        super().__init__()\n",
        "        self.x_to_dim = x_to_dim\n",
        "        self.x_from_dim = x_from_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_heads = n_heads\n",
        "        self.he_dim = hidden_dim // n_heads\n",
        "\n",
        "        self.w_query = nn.Linear(x_to_dim, hidden_dim)\n",
        "        self.w_key = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.w_value = nn.Linear(x_from_dim, hidden_dim)\n",
        "        self.linear = nn.Linear(hidden_dim, hidden_dim)\n",
        "\n",
        "    def forward(self, x_to, x_from):\n",
        "        # x_to = [batch size, x_to_len, x_to_dim]\n",
        "        # x_from = [batch size, x_from_len, x_from_dim]\n",
        "\n",
        "        batch_size = x_to.shape[0]\n",
        "        q = self.w_query(x_to)\n",
        "        k = self.w_key(x_from)\n",
        "        v = self.w_value(x_from)\n",
        "\n",
        "        # separation des vecteurs d'entrée\n",
        "        q = q.view(batch_size, -1, self.n_heads, self.he_dim).transpose(1, 2)\n",
        "        k = k.view(batch_size, -1, self.n_heads, self.he_dim).transpose(1, 2)\n",
        "        v = v.view(batch_size, -1, self.n_heads, self.he_dim).transpose(1, 2)\n",
        "\n",
        "        A = torch.einsum('bqik,bqjk->bqij', q, k) / math.sqrt(self.he_dim)\n",
        "        A = nn.Softmax(dim=-1)(A)\n",
        "        A = torch.einsum('bqki,bqkj->bqij', A, v)\n",
        "        # concaténation des attentions et projection\n",
        "        A = A.transpose(1, 2).reshape(batch_size, -1, self.hidden_dim)\n",
        "        A = self.linear(A)\n",
        "        return A\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uBJSGC6N80f1"
      },
      "outputs": [],
      "source": [
        "attention = MultiHeadAttention(4,4,10,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_hvSa9sm84Jx",
        "outputId": "8d374861-f97e-4a32-87d5-4bf29f4c6716"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.9453,  0.1184,  0.8194, -1.0045,  0.1303, -1.6511, -0.6159,\n",
              "          -0.3407, -0.9195,  0.2212],\n",
              "         [ 1.0536, -0.2872,  0.7614, -1.4821, -0.1132, -1.9304, -0.9134,\n",
              "          -0.1743, -0.7936,  0.4989]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "attention(x, x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d05Is8HdkZId"
      },
      "source": [
        "MultiheadAttention is the attention that is used in transformers in pratice. It is used in 2 flavors:\n",
        "- Self Attention: When $X_{to}$ attends itself ($X_{to}=X_{from}$)\n",
        "- Cross Attention. $X_{to}\\neq X_{from}$\n",
        "\n",
        "\n",
        "##### Question 3: Implement MultiHead Self Attention and MultiHeadCrossAttention from Multihead attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rsu8u3A0kZIe"
      },
      "outputs": [],
      "source": [
        "class MultiheadSelfAttention(nn.Module):\n",
        "  def __init__(self, x_to_dim, hidden_dim, n_heads):\n",
        "    super().__init__()\n",
        "    self.MHSAttention = MultiHeadAttention(x_to_dim, x_to_dim,hidden_dim,n_heads)\n",
        "\n",
        "  def forward(self, x_to):\n",
        "    return self.MHSAttention(x_to,x_to)\n",
        "\n",
        "class MultiheadCrossAttention(nn.Module):\n",
        "  def __init__(self, x_to_dim,x_from_dim, hidden_dim, n_heads):\n",
        "    super().__init__()\n",
        "    self.MHCAttention = MultiHeadAttention(x_to_dim,x_from_dim, hidden_dim, n_heads)\n",
        "\n",
        "  def forward(self,x_to,x_from):\n",
        "    return self.MHCAttention(x_to, x_from)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JQ_gMBKcJq2j"
      },
      "outputs": [],
      "source": [
        "mhsa = MultiheadSelfAttention(4,10,2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G0n3r2m-J3f3",
        "outputId": "593d7036-3de7-48f5-bf0e-73c19715e1da"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[[ 0.3547, -0.5814, -0.4119,  0.2062, -0.1709,  0.3415,  0.4629,\n",
              "           0.0819, -0.1023,  0.1509],\n",
              "         [ 0.7639, -1.7763, -0.5130,  0.1048, -0.0878,  0.6728,  1.6058,\n",
              "           0.9961, -0.6688,  0.1572]]], grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "mhsa(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAuSu-PHkZIe"
      },
      "source": [
        "### LayerNorm\n",
        "Another key component of the transformer is the LayerNorm. As we have previously seen, normalizing the output of a deep learning layer helps a lot with convergence and stability.\n",
        "Until Transformers, the most used normalization is BatchNorm. We normalize the data among the batch dimension. However, this has a few problems.\n",
        "- The normalization depend on the other samples in the batch\n",
        "- When using multiple GPUs, BatchNorm needs to synchronize the batch statistic across GPUs, which locks the forward process and slow down training.\n",
        "\n",
        "The last element is the most important one. Transformers, aims to be a easy to parralilize architecture and can't afford to use batchnorm.\n",
        "\n",
        "Instead, Transformers uses Layer Norm. LayerNorm is sample dependent, which removes the synchronization issue. We normalize over the channel dimension instead of the batch dimension.\n",
        "\n",
        "<img src=\"https://production-media.paperswithcode.com/methods/Screen_Shot_2020-05-19_at_4.24.42_PM.png\">\n",
        "\n",
        "To account for the loss of capacity, we map the output by a linear transformation with a learned bias and scale.\n",
        "\n",
        "##### Question 4:\n",
        "Implement the LayerNorm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hy4NpvW_kZIf"
      },
      "outputs": [],
      "source": [
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, size, eps=1e-6):\n",
        "        super().__init__()\n",
        "        self.eps = eps\n",
        "        self.w = torch.nn.Parameter(torch.ones(size))\n",
        "        self.b = torch.nn.Parameter(torch.zeros(size))\n",
        "\n",
        "    def forward(self, y):\n",
        "        x=y\n",
        "        mean = x.mean(dim=-1, keepdim=True)\n",
        "        std = x.std(dim=-1, keepdim=True, unbiased=False)\n",
        "        x = (x - mean) / (std + self.eps)\n",
        "        x = self.w * x + self.b\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "waS5pqtVkZIg"
      },
      "source": [
        "### Feed Feedward Network\n",
        "\n",
        "Finally, the last block is a feed-forward network with one hidden layer. This layer has usually a size of $2 * input\\_dim$. This is followed by a dropout layer and an activation function. Here, we will use leaky relu, with a leak parameter of 0.1.\n",
        "##### Question 5: Implement the FFN layer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VVCl3-AkZIh"
      },
      "outputs": [],
      "source": [
        "class FFN(nn.Sequential):\n",
        "    def __init__(self, hidden_dim, dropout_rate=0.1, expansion_factor=2):\n",
        "        super().__init__(\n",
        "            nn.Linear(hidden_dim, hidden_dim * expansion_factor),\n",
        "            nn.Linear(hidden_dim*expansion_factor, hidden_dim),\n",
        "            nn.Dropout(dropout_rate),\n",
        "            nn.LeakyReLU(0.1)\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWV_LlNmkZIh"
      },
      "source": [
        "### The Transformer block\n",
        "\n",
        "The last thing that we are missing are the skip connection. Like in ResNet, the transformer architecture implements the skip-connection. This allow for a better gradient flow avoiding vanishing gradient.\n",
        "There is a skip connection after the attention and the feed forward network\n",
        "\n",
        "##### Question 6.\n",
        "Looking at the transformer figure, implement the Transformer Encoder Block\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uCWRqki2kZIi"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoderBlock(nn.Module):\n",
        "    def __init__(self,data_dim, hidden_dim, n_heads, dropout_rate=0.1):\n",
        "       # To complete\n",
        "       super().__init__()\n",
        "       self.h_dim = hidden_dim\n",
        "       self.dropout_rate = dropout_rate\n",
        "       self.MHSAttention = MultiheadSelfAttention(data_dim, hidden_dim, n_heads)\n",
        "       self.norm = LayerNorm(hidden_dim)\n",
        "       self.ffn = FFN(hidden_dim, dropout_rate)\n",
        "\n",
        "    def forward(self, y):\n",
        "        # x = [batch size, x_len, hidden dim]\n",
        "        x = y\n",
        "        x0 = self.MHSAttention(x)\n",
        "        x = x0 +x\n",
        "        x = self.norm(x)\n",
        "        x0 = self.ffn(x)\n",
        "        x = x0 + x\n",
        "        x = self.norm(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHYUa9cfkZIj"
      },
      "source": [
        "### Positional embedding\n",
        "The transformers architecture is permutation independent. That means that for every token, we can swap 2 tokens and have the exact same result. However, the position of the token can be a very important information to consider. Imagine in an image. If a pixel is nearby another pixel, we want the transformer to be able to capture such information. Which is not the case for now.\n",
        "That's why we introduce positional encodings. For each token, add the positional encoding to the original token:\n",
        "\n",
        "$$\n",
        "X_i = X_i + PE(i)\n",
        "$$\n",
        "\n",
        "with X_i the token at the i dimension.\n",
        "\n",
        "The most used positional encodings are sinusoidal encodings. They are defined as follow:\n",
        "\n",
        "$$\n",
        "PE(i, 2j) = sin(i / 10000^{\\frac{2j}{d}}) \\\\\n",
        "PE(i, 2j + 1) = cos(i / 10000^{\\frac{2j}{d}})\n",
        "$$\n",
        "\n",
        "Where $d$ the dimension of the tokens, $i$, the i-th token in the sequence and $2j$ (resp $2j + 1$), the index of the dimension of the vector.\n",
        "The idea here is that we add a sinusoidal that encode the position in a multidimensional array.\n",
        "\n",
        "Another common positional encodings is the learned positional encoding. Simply, we let the network learn a set of tensor $PE$ that match the sequence length and dimension of the tokens.\n",
        "\n",
        "##### Question 7.\n",
        "\n",
        "Implement both Sinusoidal and Learned positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tv5gRVu-kZIk"
      },
      "outputs": [],
      "source": [
        "class SinusoidalPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self,x):\n",
        "      batch_size, len, h_dim = x.size()\n",
        "      position = torch.arange(0,len).unsqueeze(-1)\n",
        "      div_term = torch.exp(torch.arange(0, self.hidden_dim, 2).float() * (-math.log(10000.0) / self.hidden_dim))\n",
        "      pe = torch.zeros(1,len,self.hidden_dim)\n",
        "      pe[0,:, 0::2] = torch.sin(position * div_term)\n",
        "      pe[0,:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "      return x + pe.to(device)\n",
        "\n",
        "class LearnedPositionalEncoding(nn.Module):\n",
        "    def __init__(self, hidden_dim, max_len):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        self.param = nn.Parameter(torch.zeros((max_len,hidden_dim)))\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "      bs, len, h_dim = x.size()\n",
        "      lpe = self.param[:len, :].unsqueeze(0)\n",
        "      return x + lpe\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OWlaiH2HOHNy",
        "outputId": "e093ef8f-351e-4e67-98a8-9bbb0cd715a4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0, 1, 2, 3, 4, 5])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "x = torch.arange(6)\n",
        "x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lP5YNyq_OMl3",
        "outputId": "f315ce90-1aae-4232-d41d-95489c7fd914"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.],\n",
              "        [0., 0.]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "torch.zeros(1, 4,5)[0,:,1::2]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLUzowBkkZIl"
      },
      "source": [
        "### The transformer encoder\n",
        "Now you have everything you need to implement the transformer . You add positional encoding to the tokens and then stack N transformer encoder layers\n",
        "\n",
        "##### Question 8.\n",
        "Implement the transformer encoder with n_layers and the ability to choose both positional embeddings.\n",
        "\n",
        "Tip: Look into `ModuleList`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G2pm2XN0kZIm"
      },
      "outputs": [],
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "    def __init__(self, data_dim,  hidden_dim, n_heads, n_layers, dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "\n",
        "        if positional_encoding == \"sinusoidal\":\n",
        "          self.pe = SinusoidalPositionalEncoding(data_dim)\n",
        "        else :\n",
        "          self.pe = LearnedPositionalEncoding(data_dim, max_len)\n",
        "\n",
        "        self.transformers = nn.ModuleList([TransformerEncoderBlock(data_dim if i == 0 else hidden_dim ,hidden_dim, n_heads, dropout_rate = dropout_rate)\n",
        "            for i in range(n_layers)])\n",
        "\n",
        "\n",
        "    def forward(self, y):\n",
        "        # To complete\n",
        "        x = self.pe(y)\n",
        "        for transformer_block in self.transformers:\n",
        "          x = transformer_block(x)\n",
        "        return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb_alEFtkZIn"
      },
      "source": [
        "## The Vision Transformer\n",
        "The above architecture was introduced in 2017 to process sequences of text tokens. However, it could be useful to be able to leverage this architecture for computer vision. On the contrary of convolutional neural network, the transformer has the advantage to introduce less inductive bias.\n",
        "\n",
        "This could be interesting to leverage to improve vision systems. If we learn the biases from the data, we can hope to have better performances. We however need compute and a lot of data to do this.\n",
        "\n",
        "To apply the transformer to images, one key question remains to be answered: How do we transform an image to tokens? The approach introduce in Vision Transformers is to cut the image into patches that are then transformed into a token trhought a linear projection.\n",
        "\n",
        "We also add an extra token, known as the classification token, that will be the token which will be use to predict upon. After going through the N transformer layers, this is the token that goes throught a multi layer perceptron.\n",
        "\n",
        "\n",
        "<img src= \"https://1.bp.blogspot.com/-_mnVfmzvJWc/X8gMzhZ7SkI/AAAAAAAAG24/8gW2AHEoqUQrBwOqjhYB37A7OOjNyKuNgCLcBGAsYHQ/s1600/image1.gif\" width=\"512\">\n",
        "\n",
        "\n",
        "##### Question 9\n",
        "\n",
        "Implement the vision transformer\n",
        "\n",
        "Hint: Use Conv2D with the right kernel size and stride to do the linear projection of non-overlapping patches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E3EJAHNDkZIo"
      },
      "outputs": [],
      "source": [
        "class ViT(nn.Module):\n",
        "    def __init__(self, patch_size, hidden_dim, n_heads, n_layers, n_classes, dropout_rate=0.1, positional_encoding=\"sinusoidal\", max_len=1000):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        self.projection = nn.Conv2d(3,hidden_dim, kernel_size=patch_size, stride=patch_size)\n",
        "        self.transformerEncoder = TransformerEncoder(patch_size**2*hidden_dim, hidden_dim,n_heads, n_layers, dropout_rate, positional_encoding, max_len)\n",
        "        self.norm = nn.LayerNorm(hidden_dim)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim,n_classes))\n",
        "\n",
        "    def forward(self, y):\n",
        "        # x = [batch size, 3, image height, image width]\n",
        "        # To complete\n",
        "        x = self.linear_projection(y)\n",
        "        x = x.flatten(2)\n",
        "        x = x.permute(0, 2, 1)\n",
        "        x = self.transformer(x)\n",
        "        x = x.mean(dim=1)\n",
        "        x = self.norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vdv_i9NkZIp"
      },
      "source": [
        "### Compact Convolutional Transformer\n",
        "The previous network is a network that need a lot of compute and data to be trained. As we mentionned before, the transformer removes the inductive bias of convnets which requires more data to be tuned.\n",
        "For this TP, we will try to train an hybrid architecture that preserves the inductive biases of convolution but manages to use the transformer to add global learning.\n",
        "\n",
        "The first change is the tokenizer. We replace it with a ConvNet. Each convnet layer has a convolution, ReLU and maxpooling.\n",
        "The second change is to actually remove the classfication token and classify on top of a pooling of all tokens. The pooling is done with an attention like mechanism:\n",
        "- For each sample, we predict a scalar, that we compute the softmax over all the sample tokens.\n",
        "- We then do an weighted average pool by this softmax values over the tokens. The weight is given by the previous step\n",
        "\n",
        "More details see: https://arxiv.org/abs/2104.05704\n",
        "\n",
        "<img src= https://miro.medium.com/v2/resize:fit:720/format:webp/1*8diH01Fl7MhHRemLy9hUHw.png width=512>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBLvFB1dkZIp"
      },
      "source": [
        "##### Question 10\n",
        "Implement the Convolutional based tokenizer and the SeqPool operationm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxjXDuz-kZIq"
      },
      "outputs": [],
      "source": [
        "class ConvPatchEmbedding(nn.Module):\n",
        "    def __init__(self, n_layers, kernel_size, hidden_dim):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        initial_layer = nn.Sequential(nn.Conv2d(3, hidden_dim, kernel_size, stride= 1, padding = kernel_size//2),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size))\n",
        "        list_layers = [initial_layer] + [nn.Sequential(\n",
        "                nn.Conv2d(hidden_dim, hidden_dim, kernel_size, stride= 1, padding = kernel_size//2),\n",
        "                nn.ReLU(),\n",
        "                nn.MaxPool2d(kernel_size)\n",
        "            )\n",
        "            for i in range(n_layers-1)]\n",
        "        self.conv_nets = nn.ModuleList(list_layers)\n",
        "\n",
        "    def forward(self,x):\n",
        "      for conv in self.conv_nets:\n",
        "            x = conv(x)\n",
        "      x = x.flatten(2).transpose(1, 2)\n",
        "      return x\n",
        "\n",
        "\n",
        "class SeqPool(nn.Module):\n",
        "    def __init__(self, hidden_dim):\n",
        "        # To complete\n",
        "        super().__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.linear = nn.Linear(hidden_dim,1)\n",
        "\n",
        "    def forward(self, x):\n",
        "      scalar = self.linear(x).squeeze(-1)\n",
        "      w = nn.functional.softmax(scalar, dim=1)\n",
        "      pooled = torch.einsum('bsh,bs->bh', x, w)\n",
        "      return pooled\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJnDnNwekZIr"
      },
      "source": [
        "##### Question 11\n",
        "\n",
        "Implement the Compact Convolutional Transformer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8PlNsoiZkZIs"
      },
      "outputs": [],
      "source": [
        "class CCT(nn.Module):\n",
        "    def __init__(self, n_conv_layers, kernel_size,  n_transformer_layers, hidden_dim, n_heads, n_classes, dropout_rate=0.1):\n",
        "      super().__init__()\n",
        "      self.conv = ConvPatchEmbedding(n_conv_layers, kernel_size, hidden_dim)\n",
        "      self.transformerEncoder = TransformerEncoder(hidden_dim ,  hidden_dim , n_heads, n_transformer_layers, dropout_rate)\n",
        "      self.seqPool = SeqPool(hidden_dim)\n",
        "      self.mlpHead = nn.Sequential(\n",
        "          nn.LayerNorm(hidden_dim),\n",
        "          nn.Linear(hidden_dim,hidden_dim),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(hidden_dim, n_classes)\n",
        "      )\n",
        "\n",
        "    def forward(self, y):\n",
        "      x = self.conv(y)\n",
        "      x = self.transformerEncoder(x)\n",
        "      x = self.seqPool(x)\n",
        "      x = self.mlpHead(x)\n",
        "      return x\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QMjgxrqXkZIt"
      },
      "source": [
        "##### Question 12\n",
        "Train the CCT on CIFAR-10 for 300 epochs and log both train and test loss and accuracy. You should obtain at least 80+% test accuracy. (Possible to get 90%+).\n",
        "We provide a data augmentation strategy called auto augment to avoid overfitting on the training data.\n",
        "Hparameters are to be choosen to your discretion.\n",
        "\n",
        "Tips for Hparams:\n",
        "- Don't use too big of a transformer hidden dim (<256)\n",
        "- For the convnet, aim to have between 32 and 128 output tokens.\n",
        "- Use AdamW with some weight decay to avoid overfitting\n",
        "- Use between 2 and 6 transformer layers.\n",
        "- Use between 2 and 4 transformer heads\n",
        "\n",
        "Training takes around 30min (depending of hparams), so keep working on the next questions while it trains. You can copy paste the notebook and run it in a separate collab instance to be able to execute the code of the next questions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o7tEHJAlkZIt",
        "outputId": "6ba552ea-5c9f-4df0-e661-d2a6ac29864b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:03<00:00, 45027005.11it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/torch/utils/data/dataloader.py:561: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_set = CIFAR10(root='./data', train=True, download=True, transform=transforms.Compose([\n",
        "    transforms.autoaugment.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=4)\n",
        "\n",
        "test_set = CIFAR10(root='./data', train=False, download=True, transform=transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2470, 0.2435, 0.2616))\n",
        "]))\n",
        "\n",
        "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, num_workers=4)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "phJzvcJnofv-",
        "outputId": "e4dd10c9-9259-4cb7-e061-83b1539a898a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[[ 1.7416,  1.6781,  1.6939,  ...,  1.7098,  1.7098,  1.6939],\n",
            "         [ 1.7892,  1.7416,  1.7416,  ...,  1.7575,  1.7575,  1.7416],\n",
            "         [ 1.7733,  1.7257,  1.7257,  ...,  1.7416,  1.7416,  1.7257],\n",
            "         ...,\n",
            "         [-0.6082, -1.3068, -1.6878,  ...,  0.6937,  0.9001,  0.9954],\n",
            "         [-0.6876, -1.2591, -1.4179,  ...,  0.7731,  0.9477,  0.9795],\n",
            "         [-0.6399, -1.0051, -1.0686,  ...,  0.6778,  0.8683,  0.9636]],\n",
            "\n",
            "        [[ 1.8044,  1.7400,  1.7561,  ...,  1.7722,  1.7722,  1.7561],\n",
            "         [ 1.8527,  1.8044,  1.8044,  ...,  1.8205,  1.8205,  1.8044],\n",
            "         [ 1.8366,  1.7883,  1.7883,  ...,  1.8044,  1.8044,  1.7883],\n",
            "         ...,\n",
            "         [-0.3859, -1.1589, -1.6099,  ...,  0.9830,  1.1924,  1.2729],\n",
            "         [-0.4342, -1.0623, -1.2717,  ...,  1.0636,  1.2407,  1.2729],\n",
            "         [-0.3537, -0.7724, -0.9013,  ...,  0.9669,  1.1602,  1.2407]],\n",
            "\n",
            "        [[ 1.8160,  1.7560,  1.7710,  ...,  1.7860,  1.7860,  1.7710],\n",
            "         [ 1.8610,  1.8160,  1.8160,  ...,  1.8310,  1.8310,  1.8160],\n",
            "         [ 1.8460,  1.8010,  1.8010,  ...,  1.8160,  1.8160,  1.8010],\n",
            "         ...,\n",
            "         [-0.3726, -1.1521, -1.5419,  ...,  0.9765,  1.1864,  1.3063],\n",
            "         [-0.4776, -1.1671, -1.3770,  ...,  1.0365,  1.2314,  1.2913],\n",
            "         [-0.4626, -0.9873, -1.1372,  ...,  0.9615,  1.1564,  1.2763]]])\n"
          ]
        }
      ],
      "source": [
        "for i,test in enumerate(test_loader):\n",
        "  print(test[0][1])\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DKlzO2L9EeuM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vdEcGJp3kZIu"
      },
      "outputs": [],
      "source": [
        "def train(model, device, dataloader, epoch, rate = 1e-4):\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=rate)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "    train_losses = []\n",
        "    for t in tqdm(range(epoch)):\n",
        "        for i, (input_data, target) in enumerate(dataloader):\n",
        "            input_data, target = input_data.to(device), target.to(device)\n",
        "            y_pred = model(input_data)\n",
        "            loss = loss_fn(y_pred, target)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "        train_losses.append(loss.detach().item())\n",
        "\n",
        "    return train_losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mXUoYWhnEqWg"
      },
      "outputs": [],
      "source": [
        "cct = CCT(2, 2 , 2, 64, 2, 10, dropout_rate=0.1).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vu_Ewxv5kZIv",
        "outputId": "29230993-c7c0-47b5-d601-87ce3e32953c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 60/60 [32:03<00:00, 32.06s/it]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.9476810693740845,\n",
              " 1.8285179138183594,\n",
              " 1.7229106426239014,\n",
              " 1.553041696548462,\n",
              " 1.5502225160598755,\n",
              " 1.5303542613983154,\n",
              " 1.5699392557144165,\n",
              " 1.5757817029953003,\n",
              " 1.5357153415679932,\n",
              " 1.5646289587020874,\n",
              " 1.4238452911376953,\n",
              " 1.5295069217681885,\n",
              " 1.439213514328003,\n",
              " 1.1810235977172852,\n",
              " 1.2987089157104492,\n",
              " 1.519635796546936,\n",
              " 1.4135290384292603,\n",
              " 1.2578595876693726,\n",
              " 1.5348069667816162,\n",
              " 1.4834283590316772,\n",
              " 1.6704965829849243,\n",
              " 1.3717098236083984,\n",
              " 1.5442084074020386,\n",
              " 1.3372304439544678,\n",
              " 1.2268873453140259,\n",
              " 1.62014639377594,\n",
              " 1.2977235317230225,\n",
              " 1.4628371000289917,\n",
              " 1.3077534437179565,\n",
              " 1.3193362951278687,\n",
              " 1.4504109621047974,\n",
              " 1.0950582027435303,\n",
              " 1.2305861711502075,\n",
              " 1.3158506155014038,\n",
              " 1.340714931488037,\n",
              " 1.1126363277435303,\n",
              " 1.3511735200881958,\n",
              " 1.1501401662826538,\n",
              " 1.1959095001220703,\n",
              " 1.3208059072494507,\n",
              " 1.1327016353607178,\n",
              " 1.2234971523284912,\n",
              " 1.3177011013031006,\n",
              " 1.2000446319580078,\n",
              " 1.3073893785476685,\n",
              " 1.0386688709259033,\n",
              " 1.0902806520462036,\n",
              " 1.1748830080032349,\n",
              " 1.1213034391403198,\n",
              " 1.2905210256576538,\n",
              " 1.3356475830078125,\n",
              " 1.1738591194152832,\n",
              " 1.1185718774795532,\n",
              " 1.2375727891921997,\n",
              " 1.2441668510437012,\n",
              " 1.195361614227295,\n",
              " 1.0166451930999756,\n",
              " 1.210510492324829,\n",
              " 1.162841558456421,\n",
              " 1.22185218334198]"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ],
      "source": [
        "train(cct, device, train_loader, epoch = 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YPKHeilyjJiv"
      },
      "outputs": [],
      "source": [
        "def success_rate(model,dataloader, batch_size = batch_size):\n",
        "  nb = 0\n",
        "  success = 0\n",
        "  for i, (input, target) in enumerate(dataloader):\n",
        "    input, target = input.to(device), target.to(device)\n",
        "    with torch.no_grad():\n",
        "      y_pred = model(input)\n",
        "    pred = torch.argmax(y_pred, dim = -1)\n",
        "    for i in range(int(pred.shape[0])):\n",
        "      if pred[i] == target[i]:\n",
        "        success += 1\n",
        "      nb += 1\n",
        "  return success / nb"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "success_rate(cct,test_loader)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eL-h1I9NQo3s",
        "outputId": "f948cc7a-a6f2-46cb-ad15-8ce04b0994d6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6522"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training on 60 epochs in 30 minutes with GPU Tesla 4. Success rate of 65.22%"
      ],
      "metadata": {
        "id": "bWzxs-FwckL0"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I0LQCXrtkZIv"
      },
      "source": [
        "## What is my transformer doing? Visualizing the attention matrices\n",
        "Transformers offer a great tool for visualisation. Indeed, we can look at the attention matrices to see what is our attention block looking at. This allows to visualise what data is the transformer paying attention. It could be super useful to identify biases on which the network has been focusing.\n",
        "\n",
        "Imagine you want to classify dogs and cats, but in your training data dogs always have a red collar. When you use your classifier on a cat with a red collar it classifies it as a dog! Looking at the attention matrix you can see that the transformer just look at the cats collar and doesn't pay attention to cat itself. You just have identified a bias in your data! You can now fix it by collecting data of dogs without collars or of cats with red collars.\n",
        "\n",
        "\n",
        "The idea here is to look at the attention matrices. We will look into a pretrained ViT called DiNO. Dino has been train with a self-supervised training. We will visualize the attention matrices of this network for some images.\n",
        "\n",
        "We will use the timm library which has lots of models implemented with pretrained weights.\n",
        "\n",
        "First let's list all the models that have been trained with the dino procedure:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ue0PxtrHkZIw",
        "outputId": "74521009-1950-40bc-ef3c-4db224d4bea9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['vit_base_patch8_224_dino',\n",
              " 'vit_base_patch16_224_dino',\n",
              " 'vit_small_patch8_224_dino',\n",
              " 'vit_small_patch16_224_dino']"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "timm.list_models('*vit*dino*')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FRKprbPGkZIw",
        "outputId": "060ad893-24bb-4d8a-ed04-d45f77510593"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/dino/dino_vitbase8_pretrain/dino_vitbase8_pretrain.pth\" to /root/.cache/torch/hub/checkpoints/dino_vitbase8_pretrain.pth\n"
          ]
        }
      ],
      "source": [
        "dino = timm.create_model('vit_base_patch8_224_dino', pretrained=True, img_size=480).eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk4oq1z4kZIx"
      },
      "source": [
        "We will use the torchvision to extract the attention matrix. Look at this tutorial on how to extract certain node in the computational network of a model: https://pytorch.org/vision/stable/feature_extraction.html\n",
        "\n",
        "##### Question 13\n",
        "First, isolate for each block the name of the node which correspond to the attention matrix.\n",
        "To guide you, you can look at the Timm library implementation of ViT.\n",
        "\n",
        "https://github.com/huggingface/pytorch-image-models/blob/7501972cd61dde7428164041b0a6dd8fea60c4d4/timm/models/vision_transformer.py"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nodes = [dino.blocks[i].attn.proj for i in range(len(dino.blocks))]"
      ],
      "metadata": {
        "id": "FET5HVPkSOKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlT59A18kZIy"
      },
      "source": [
        "##### Question 14\n",
        "Create the feature extractor that outputs all the attention matrices"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_attention_maps(x):\n",
        "  attention_maps = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "        x = dino.patch_embed(x)\n",
        "        x = dino.pos_drop(x)\n",
        "        x = dino.norm_pre(x)\n",
        "        for i, blk in enumerate(dino.blocks):\n",
        "            x = blk(x)\n",
        "            attention_maps.append(torch.softmax(blk.attn.proj.parameters(), dim=-1))\n",
        "\n",
        "    # Return the attention matrices\n",
        "  return attention_maps"
      ],
      "metadata": {
        "id": "rEolsidVSG9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d7kfE9q9kZIz"
      },
      "source": [
        "##### Question 15\n",
        "Now, find some images online and visualize the attention matrices. Look for images with multiple objects.\n",
        "We will visualize the matrix corresponding to the class token with all the other tokens. Make sure to reshape them so that they have an image format. Plot every block head attention matrix in a single row. Comment. Plot also the last layer attentions superoposed on the real images.\n",
        "\n",
        "We provide the code to process the image from an image url.\n",
        "\n",
        "Tip: For easy reshaping of tensor, look into the `einops` library.\n",
        "\n",
        "Tip 2: To better visualize the post softmax attention, you can clamp the values and renormalize. Otherwise, a single token can have too much attention and will not allow to visualize the rest of the tokens."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R0H9N4lkZI0"
      },
      "outputs": [],
      "source": [
        "\n",
        "def get_img_from_url(url):\n",
        "    response = requests.get(url)\n",
        "    img = Image.open(BytesIO(response.content))\n",
        "    img = img.convert('RGB')\n",
        "    img = img.resize((480, 480))\n",
        "    img = transforms.ToTensor()(img)\n",
        "    img = transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))(img)\n",
        "    img = img.unsqueeze(0)\n",
        "    return img"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1yFLrxiwkZI1"
      },
      "outputs": [],
      "source": [
        "# To complete"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE9GYobTkZI1"
      },
      "outputs": [],
      "source": [
        "#def plot_attn_matrix(attn_matrices):\n",
        "    # To complete\n",
        "\n",
        "#plot_attn_matrix(attn_matrices)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BMCn3xkhkZI3"
      },
      "outputs": [],
      "source": [
        "# Overlapp the attention matrix with the image (only the last block)\n",
        "#def plot_attn_matrix_with_image(attn_matrices, img):\n",
        "    # To complete\n",
        "\n",
        "#plot_attn_matrix_with_image(attn_matrices, img)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "I0LQCXrtkZIv"
      ],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "84fe68c32616dcb624e285180337c2501cb5688042d5b0690c59ead12741ee4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}